# -*- coding: utf-8 -*-
"""Soil_moisture2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OQUdD9HUV6bDWE6NXGv6iS_XoVr30aH3
"""

import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from scipy.interpolate import griddata
import numpy as np

import os
import matplotlib.pyplot as plt

# Load both datasets
csv_path1 = "plant_vase1.CSV"  # Replace with actual path
csv_path2 = "plant_vase1(2).CSV"  # Replace with actual path

df1 = pd.read_csv(csv_path1)
df2 = pd.read_csv(csv_path2)

# Compare column names
print("Dataset 1 Columns:", df1.columns)
print("Dataset 2 Columns:", df2.columns)

# Check shape (number of rows, columns)
print("Dataset 1 Shape:", df1.shape)
print("Dataset 2 Shape:", df2.shape)

# Check for missing values
print("Missing values in Dataset 1:\n", df1.isnull().sum())
print("Missing values in Dataset 2:\n", df2.isnull().sum())

# Display first few rows from each dataset
print("First few rows of Dataset 1:\n", df1.head())
print("First few rows of Dataset 2:\n", df2.head())

# Load your datasets (replace with your actual paths if needed)
df1 = pd.read_csv("plant_vase1.CSV")
df2 = pd.read_csv("plant_vase1(2).CSV")

# Combine both
df = pd.concat([df1, df2], ignore_index=True)

# Convert to Timestamp
df['Timestamp'] = pd.to_datetime(df[['year', 'month', 'day', 'hour', 'minute', 'second']])

# Optional: Sort by time
df = df.sort_values('Timestamp').reset_index(drop=True)

# Drop original date columns (optional)
df.drop(['year', 'month', 'day', 'hour', 'minute', 'second'], axis=1, inplace=True)

# Normalize moisture values to [0, 1] if not already normalized
moisture_cols = [col for col in df.columns if "moisture" in col]
df[moisture_cols] = df[moisture_cols].apply(lambda x: (x - x.min()) / (x.max() - x.min()))

# Print sample
print(df.head())

# Calculate average moisture from all 5 sensors
df['avg_moisture'] = df[['moisture0', 'moisture1', 'moisture2', 'moisture3', 'moisture4']].mean(axis=1)
#normalized volumetric water content (VWC)
# Define landslide risk
df['landslide'] = (df['avg_moisture'] > 0.3)

# Optional: drop avg_moisture column if not needed
df.drop(columns=['avg_moisture'], inplace=True)

# Check balance of labels
print(df['landslide'].value_counts())

df['landslide']  # values: 0 (no landslide), 1 (landslide)
# Save the updated DataFrame to a new CSV file
df.to_csv("upMoisture_dataset.csv", index=False)

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Features and labels
X = df[moisture_cols].values.astype('float32')
y = df['landslide'].astype(int).values  # Convert boolean to int (False=0, True=1)

# Split into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert to PyTorch tensors
X_train = torch.tensor(X_train)
X_test = torch.tensor(X_test)
y_train = torch.tensor(y_train).long()
y_test = torch.tensor(y_test).long()

# Define MLP
class MoistureMLP(nn.Module):
    def __init__(self):
        super(MoistureMLP, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(5, 32),
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, 2)  # Output 2 classes: irrigation/no irrigation
        )

    def forward(self, x):
        return self.model(x)

mlp = MoistureMLP()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(mlp.parameters(), lr=0.001)

# Training loop
for epoch in range(50):
    optimizer.zero_grad()
    outputs = mlp(X_train)
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()

    acc = (outputs.argmax(1) == y_train).float().mean()
    print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}, Accuracy: {acc:.4f}")

import numpy as np
sensor_coords = np.array([
    [0.1, 0.1],
    [0.3, 0.7],
    [0.5, 0.3],
    [0.7, 0.8],
    [0.9, 0.2]
])

from scipy.interpolate import griddata
import numpy as np

def generate_heatmap_from_row(row, sensor_coords, grid_size=(128, 128)):
    values = np.array([
        row['moisture0'],
        row['moisture1'],
        row['moisture2'],
        row['moisture3'],
        row['moisture4']
    ])
    grid_size = (128, 128)
    # Prepare grid
    grid_x, grid_y = np.meshgrid(
        np.linspace(0, 1, grid_size[0]),
        np.linspace(0, 1, grid_size[1])
    )

    # Interpolate using nearest method
    heatmap = griddata(sensor_coords, values, (grid_x, grid_y), method='nearest')

    return heatmap

heatmaps = []
grid_size = (128, 128)
for _, row in df.iterrows():
    heatmap = generate_heatmap_from_row(row, sensor_coords, grid_size)
    heatmaps.append(heatmap)

heatmaps = np.array(heatmaps).astype('float32')
print("Heatmaps shape:", heatmaps.shape)

for row in df.itertuples(index=False):
    heatmap = generate_heatmap_from_row(row._asdict(), sensor_coords, grid_size)

np.save("soilmoisture_heatmaps.npy", heatmaps)

import os
import matplotlib.pyplot as plt

# Directory to store heatmaps
image_dir = "heatmap_images"
os.makedirs(image_dir, exist_ok=True)

# Count existing images
existing_images = len([
    f for f in os.listdir(image_dir)
    if f.startswith("heatmap_") and f.endswith(".png")
])
print(f"{existing_images} heatmaps already exist, generating next 1500 (or until finished)...")

# Generate the next 100 heatmaps only
for i in range(existing_images, min(existing_images + 1500, len(heatmaps))):
    plt.imshow(heatmaps[i], cmap="viridis")
    plt.axis("off")
    plt.savefig(f"{image_dir}/heatmap_{i}.png", bbox_inches="tight", pad_inches=0)
    plt.close()

print("Generation done ✅")

heatmaps = np.load("soilmoisture_heatmaps.npy")

df = pd.read_csv("upMoisture_dataset.csv")

#. Combine MLP inputs and Heatmap tensors
from sklearn.model_selection import train_test_split
import torch
import torch.nn.functional as F

# Features
mlp_inputs = df[moisture_cols].values.astype('float32')
heatmaps = np.squeeze(heatmaps)  # Remove any (N, 1, 128, 128, 1) -> (N, 128, 128)
heatmaps = np.expand_dims(heatmaps, axis=1)

# Labels
labels = df['landslide'].astype(int).values

# Train-test split
X_mlp_train, X_mlp_test, X_heat_train, X_heat_test, y_train, y_test = train_test_split(
    mlp_inputs, heatmaps, labels, test_size=0.2, random_state=42)

# Convert to tensors
X_mlp_train = torch.tensor(X_mlp_train)
X_mlp_test = torch.tensor(X_mlp_test)
X_heat_train = torch.tensor(X_heat_train)
X_heat_test = torch.tensor(X_heat_test)
y_train = torch.tensor(y_train).long()
y_test = torch.tensor(y_test).long()

import torch.nn as nn

class FusionNet(nn.Module):
    def __init__(self):
        super(FusionNet, self).__init__()

        # CNN for heatmap images
        self.cnn = nn.Sequential(
            nn.Conv2d(1, 8, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),  # 64x64
            nn.Conv2d(8, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),  # 32x32
            nn.Flatten()
        )

        # MLP for soil moisture features
        self.mlp = nn.Sequential(
            nn.Linear(5, 16),
            nn.ReLU()
        )

        # Final classifier
        self.classifier = nn.Sequential(
            nn.Linear(135424 +16, 64),  # adjust depending on CNN output
            nn.ReLU(),
            nn.Linear(64, 2)  # 2 classes: landslide or not
        )

    def forward(self, x_img, x_mlp):
        img_feat = self.cnn(x_img)
        #print("CNN Output Shape:", img_feat.shape)
        mlp_feat = self.mlp(x_mlp)
        combined = torch.cat([img_feat, mlp_feat], dim=1)
        return self.classifier(combined)

model = FusionNet()
#model.forward(x_img, x_mlp)

from torch.utils.data import Dataset
from PIL import Image
import os
import torch

class LandslideDataset(Dataset):
    def __init__(self, df, heatmap_dir, transform=None):
        self.df = df.reset_index(drop=True)
        self.heatmap_dir = heatmap_dir
        self.transform = transform

        # Extract moisture columns
        self.moisture_cols = [col for col in df.columns if "moisture" in col]
        self.labels = df['landslide'].astype(int).values

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        # Load heatmap image
        img_path = os.path.join(self.heatmap_dir, f"heatmap_{idx}.png")
        image = Image.open(img_path).convert('L')  # grayscale

        if self.transform:
            image = self.transform(image)
        else:
            image = torch.tensor(np.array(image), dtype=torch.float32).unsqueeze(0) / 255.0

        # MLP moisture inputs
        moisture_data = torch.tensor(self.df.loc[idx, self.moisture_cols].values.astype('float32'))

        # Label
        label = torch.tensor(self.labels[idx], dtype=torch.long)

        return image, moisture_data, label

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedShuffleSplit
from torch.utils.data import Subset, DataLoader
from torchvision import transforms

# 1. Setup transform
transform = transforms.Compose([
    transforms.ToTensor()
])

# 2. Create dataset
full_dataset = LandslideDataset(df, heatmap_dir='heatmap_images', transform=transform)

# 3. Labels for stratification
labels = df['landslide'].values

# 4. Stratified split
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_idx, test_idx in sss.split(np.zeros(len(labels)), labels):
    train_dataset = Subset(full_dataset, train_idx)
    test_dataset = Subset(full_dataset, test_idx)

# 5. Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32)

import torch.nn as nn
import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

EPOCHS = 20
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

for epoch in range(EPOCHS):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for heatmap_imgs, mlp_inputs, labels in train_loader:
        heatmap_imgs = heatmap_imgs.to(device)
        mlp_inputs = mlp_inputs.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = model(heatmap_imgs, mlp_inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        correct += (outputs.argmax(1) == labels).sum().item()
        total += labels.size(0)

    acc = correct / total
    avg_loss = running_loss / len(train_loader)

    print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {acc:.4f}")

# Save model weights only
torch.save(model.state_dict(), 'Fusion_Model_soilmoisture.pth')

# Save full model state (includes optimizer, epoch, etc.)
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'epoch': epoch,
}, 'Fusion_Model_checkpoint.pth')

# Load weights only
#model.load_state_dict(torch.load('fusion_model_soilmoisture.pth'))
model.load_state_dict(torch.load('Fusion_Model_soilmoisture.pth', weights_only=True))

# Load full checkpoint
checkpoint = torch.load('Fusion_Model_checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

model.eval()
correct = 0
total = 0

with torch.no_grad():
    for heatmap_imgs, mlp_inputs, labels in test_loader:
        heatmap_imgs = heatmap_imgs.to(device)
        mlp_inputs = mlp_inputs.to(device)
        labels = labels.to(device)

        outputs = model(heatmap_imgs, mlp_inputs)
        predicted = outputs.argmax(1)
        correct += (predicted == labels).sum().item()
        total += labels.size(0)

print(f"Test Accuracy: {correct*100 / total:.4f}")

# Pick a sample from the test dataset (e.g., index 0)
sample_img, sample_mlp, sample_label = test_dataset[0]  # You can change the index

# Prepare tensors
sample_img_tensor = sample_img.unsqueeze(0).to(device)  # [1, 1, 128, 128]
sample_mlp_tensor = sample_mlp.unsqueeze(0).to(device)  # [1, 5]

# Run inference
model.eval()
with torch.no_grad():
    output = model(sample_img_tensor, sample_mlp_tensor)
    predicted_class = torch.argmax(output, dim=1).item()

# Output result
classes = ['Non-Landslide', 'Landslide']
print("Predicted:", classes[predicted_class])
print("Actual   :", classes[sample_label.item()])

from PIL import Image
import numpy as np
import torch

# Example index (change this to any valid index in your dataset)
idx = 10

# Load image
img_path = f"heatmap_images/heatmap_{idx}.png"
image = Image.open(img_path).convert('L')
image_np = np.array(image, dtype=np.float32) / 255.0
heatmap_img_tensor = torch.tensor(image_np).unsqueeze(0).unsqueeze(0)  # shape: [1, 1, 128, 128]

# Load MLP input
moisture_cols = [col for col in df.columns if 'moisture' in col]
mlp_input_np = df.loc[idx, moisture_cols].values.astype('float32')
mlp_tensor = torch.tensor(mlp_input_np).unsqueeze(0)  # shape: [1, 5]

# Move to device
sample_img = heatmap_img_tensor.to(device)
sample_mlp = mlp_tensor.to(device)

# Predict
model.eval()
with torch.no_grad():
    output = model(sample_img, sample_mlp)
    predicted_class = torch.argmax(output, dim=1).item()

print("Predicted:", "Landslide" if predicted_class == 1 else "Non-Landslide")

from sklearn.metrics import classification_report, confusion_matrix

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for imgs, mlp_inputs, labels in test_loader:
        imgs = imgs.to(device)
        mlp_inputs = mlp_inputs.to(device)
        labels = labels.to(device)

        outputs = model(imgs, mlp_inputs)
        preds = torch.argmax(outputs, dim=1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Report
print("\nClassification Report:")
print(classification_report(all_labels, all_preds, target_names=['Non-Landslide', 'Landslide']))

print("Confusion Matrix:")
print(confusion_matrix(all_labels, all_preds))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Compute confusion matrix
cm = confusion_matrix(all_labels, all_preds)

# Optional: Define class names
class_names = ['Non-Landslide', 'Landslide']  # Change based on your classes

# Plot using seaborn heatmap
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

#TESTING
from PIL import Image
import numpy as np
import torch

# Sample random unseen indices (change range or use test split if available)
test_indices = [100, 105, 110, 120, 125]  # Replace with actual unseen/test indices

model.eval()

for idx in test_indices:
    try:
        # Load heatmap image
        img_path = f"heatmap_images/heatmap_{idx}.png"
        image = Image.open(img_path).convert('L')
        image_np = np.array(image, dtype=np.float32) / 255.0
        heatmap_img_tensor = torch.tensor(image_np).unsqueeze(0).unsqueeze(0).to(device)

        # Load corresponding MLP input (moisture values)
        moisture_cols = [col for col in df.columns if 'moisture' in col]
        mlp_input_np = df.loc[idx, moisture_cols].values.astype('float32')
        mlp_tensor = torch.tensor(mlp_input_np).unsqueeze(0).to(device)

        # Get ground truth
        true_label = df.loc[idx, 'landslide']

        # Predict
        with torch.no_grad():
            output = model(heatmap_img_tensor, mlp_tensor)
            predicted_class = torch.argmax(output, dim=1).item()

        # Result
        print(f"Index: {idx}")
        print(f"Predicted: {'Landslide' if predicted_class == 1 else 'Non-Landslide'}")
        print(f"Actual   : {'Landslide' if true_label == 1 else 'Non-Landslide'}")
        print("Correct Prediction ✅" if predicted_class == true_label else "Wrong Prediction ❌")
        print("-" * 40)

    except Exception as e:
        print(f"Error at index {idx}: {e}")

print(df.columns)

import pandas as pd
import torch
from torchvision import transforms
from PIL import Image
import numpy as np
import os

# Load your model and set to eval mode
model.eval()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Load unseen CSV
unseen_df = pd.read_csv('unseen_soilmoisture_data.csv')

# Preprocessing
moisture_cols = [col for col in unseen_df.columns if "moisture" in col]
transform = transforms.Compose([
    transforms.ToTensor()  # (H, W) -> (1, H, W), scale to [0, 1]
])

# Perform predictions
for idx in range(len(unseen_df)):
    # Load heatmap image
    heatmap_path = f"heatmap_images/heatmap_{idx}.png"
    if not os.path.exists(heatmap_path):
        print(f"Heatmap image not found: {heatmap_path}")
        continue

    image = Image.open(heatmap_path).convert('L')
    image_tensor = transform(image).unsqueeze(0).to(device)  # Shape: [1, 1, H, W]

    # MLP input
    mlp_input = unseen_df.loc[idx, moisture_cols].astype('float32').values
    mlp_tensor = torch.tensor(mlp_input).unsqueeze(0).to(device)  # Shape: [1, 5]

    # Ground truth
    label = unseen_df.loc[idx, 'landslide']

    # Inference
    with torch.no_grad():
        output = model(image_tensor, mlp_tensor)
        pred = torch.argmax(output, dim=1).item()

    result = "✅ Correct" if pred == label else "❌ Incorrect"
    print(f"Sample {idx} | Prediction: {pred} | Actual: {label} | {result}")